\section{Image Preprocessing}
Dataset augmentation can be a powerful tool in image classification. By adding random translations, rotations, and reflections to a dataset, we make it harder for our model to exactly match the training data. However, the use of preprocessing must be tailored to the particular application; performing reflections on a set of handwritten digits would be harmful, for example. Performing vertical reflections on our set of upright images would be similarly unhelpful.
\subsection{Overview}
Small translations are most likely safe, and they may well be effective, as in AlexNet. It is less clear whether small rotations or horizontal reflections would be desirable. We will discard vertical reflections out of hand.

My knowledge of more sophisticated image processing techniques is limited, and I don't want to make uninformed guesses or play around with things I don't understand. However, as I progress with this project, I will also be reading up on any other methods of dataset augmentation and dimensionality reduction, and if I find something useful, it is possible that I could make my preprocessing more sophisticated.

One important thing to note about our function is that we return $28\times 28$ images. This choice is not trivial, and we may be wrong. AlexNet also performed random translations of image data, and it cropped the images. However, in comparison with AlexNet, we have the advantage that all our images are on a black background, while AlexNet was working with images whose backgrounds could not reasonably be extended. In order to ensure that we don't ``learn'' from whatever noise may be present in the black background, I provided a boolean parameter which, when {\ttfamily True}, adjusts very small pixel values to zero, with a smooth transition to the general procedure of not adjusting values at all. It is quite likely that this is entirely unnecessary, and I may eliminate this parameter from my model in early testing.
\subsection{Parameters} \label{imageprocessingparams}
In {\ttfamily augmentation.py}, we write functions for performing random transformations on a single image, and for augmenting a dataset in memory. At the moment, there are 5 parameters of {\ttfamily augmentation.randomize\_image()} that are also hyperparameters of our model. The first is {\ttfamily p\_same}, which is the probability that the function simply returns the original image unaltered. I included this parameter as a means of combating underfitting. I have observed that extreme underfitting can occur with certain values of other parameters, and one simple way to counteract this is to reduce the effective weight of randomly transformed training examples by increasing {\ttfamily p\_same}.

Two other parameters are {\ttfamily p\_rotate} and {\ttfamily p\_flip}, which are the probability of performing a random rotation and a horizontal reflection, respectively. It is quite likely that it is better to reduce the angle of random rotations than to randomly shut them off, in which case perhaps the optimal value of {\ttfamily p\_rotate} would be 1. It is also possible that random rotations are not helpful at all, in which case the optimal value is 0. For {\ttfamily p\_flip}, we will most likely start by comparing small values of {\ttfamily p\_flip} to {\ttfamily p\_flip = 0}. If {\ttfamily p\_flip = 0} consistently outperforms small positive values of  {\ttfamily p\_flip}, we will conclude that horizontal flips are not useful, and eliminate them from future consideration.

The last two parameters are {\ttfamily max\_angle} and {\ttfamily max\_shift}. {\ttfamily max\_angle} specifies the maximum angle, in degrees, that a random rotation can have, while {\ttfamily max\_shift} determines the maximum number of pixels by which we may translate the image vertically or horizontally. For rotations, we use a uniform distribution over the interval $(-\textrm{\ttfamily max\_angle},\ \textrm{\ttfamily max\_angle})$, but it could also be interesting to consider a Gaussian distribution. For translations, we use a uniform distribution over \[\{(m,n)\\,|\,m,n \in\mathbb{Z}\textrm{\ and\ }|m|,|n|\le\textrm{\ttfamily max\_shift}\}\,.\]

While other distributions could be considered for translations, I don't know that they would provide a significant advantage over the alternative of altering {\ttfamily p\_same}, especially given the small number of possible translations for the likely range of effective values for {\ttfamily max\_shift}.